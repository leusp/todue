\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

% read references.bib for the bibtex data
\bibliography{references}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (The Occidental Computer Science Comprehensive Project: Goals, Timeline, Format, and Advice)
    /Author (Justin Li)
}

% set the title and author information
\title{ToDue: A Syllabus Parsing & Deadline Automation Tool
}
\author{Princess Leus - \textbf{still working sorry :((}}
\affiliation{Occidental College}
\email{leusp@oxy.edu}

\begin{document}

\maketitle


\section{Abstract}

\section{Introduction and Problem Context}

University students often faced  turbulent and demanding academic environments, necessitating exceptional time management and organizational efficiency to successfully navigate the balance of coursework, extracurricular activities, and personal life. Students who actively use time management strategies, such as planning tools and assignment trackers, demonstrate a significant, positive correlation with higher academic performance and reduced stress \cite{macan_college_1990}. Despite the critical need for effective planning, students face a significant resource intensive hurdle at the beginning of each semester. The course syllabus presents assignment deadlines and scheduling requirements in a plethora of diverse and unstructured formats across institutions and disciplines. The lack of standardization forces students into a manual and tedious process of transcribing crucial data into personal planning tools, a task estimated to consume between 45 and 60 minutes per semester based on user interviews. 
The core problem is the lack of standardization in course syllabi, which necessitates time-consuming and error-prone manual data abstraction by students to convert unstructured assignment and scheduling requirements into functional, structured planning data.
Solving this problem provides immense value by directly addressing the need for faster, innovative solutions that fit the fast-paced nature of college life. Automating this process transforms a significant initial hurdle into a seamless digital workflow, freeing up non-productive time that can be dedicated to core studying and reducing elevated academic stress.
The reliance on archaic principles, tools, and technologies prevails longer in education compared to other growing fields. This project contributes to addressing this lag by providing a technological solution in a domain often slow to adopt automation. Given that technology facilitates PDF text analysis and extraction, and the volume of information stored in the PDF format is increasing, automating processes associated with either structure identification or specific content extraction is gaining interest. This project aims to bring faster, innovative solutions that fit the fast-paced nature of college life and increase organizational efficiency to successfully navigate academic demands.


\section{Technical Background *** add regex} 
\subsection{Problem Formulation and Data Architecture}
The project's objective is Information Extraction (IE) from course syllabi. These documents are characterized as semi-structured data because they lack the rigid schema of a database but contain discernible organization, such as tables and headings. This contrasts with unstructured data (free-flowing narrative) or fully structured data (database fields) \cite{pdf-data-extractor-chemical-info}. Unlike documents in fields like chemistry or medicine, where automated extraction is more common, educational documents like syllabi present a unique challenge due to archaic tools and lack of standardization in format. This central challenge leads to diverse and complex layouts that hinder automated extraction [\textbf{ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientific LiteratureArticle link copied!}].
IE from documents often relies on architectural patterns that manage the separation of text retrieval from semantic interpretation. A Two-Stage Architecture is commonly employed, where the initial conversion of the document to raw text (detection and recognition) is separated from the IE task.
\begin{itemize}
    \item PDF Parsing: The process begins with basic parsing, where tools are used to convert the Portable Document Format (PDF) into basic text blocks. This step does not include structural analysis or identify the logical role of text blocks.
    \item Text Serialization: Following basic parsing, text serialization is required. This  process involves ordering the extracted text blocks into a logical sequence that reflects the document flow, often needing the fusion of spatial, semantic, and visual information.
\end{itemize}

\subsubsection{Feature-Based Classification}
Modern IE, including tools like GROBID and ParsCit, often relies on classification models to assign attributes to identified text \textbf{**INSERT CITATION**}. This process depends on converting text into numerical features that the algorithms can process.
\begin{itemize}
    \item Feature Types: Features are derived from the text using various linguistic and structural properties, including orthographic, lexical, morphological, contextual, semantic, and domain-specific features.
    \item TF-IDF (Term Frequency-Inverse Document Frequency): This is a key feature extraction technique used to quantify the importance of words in a document relative to a corpus. The mathematical concept weights terms by considering:
    \begin{enumerate}
        \item Term Frequency (TF): How often a word appears in the current text block.
        \item Inverse Document Frequency (IDF): How rare the word is across the entire set of syllabi. This weighting helps the classifier identify rare, domain-specific terms (like "Midterm") while down-weighting common terms.
    \end{enumerate}
\item Conditional Random Fields (CRFs): These are powerful sequence labeling models often trained for event and temporal recognition. CRFs are used to identify relevant text spans (entities) and assign required attributes (like 'due date').
\end{itemize}
\subsubsection{Named Entity Recognition and Temporal Normalization}
\begin{itemize}
    \item Named Entity Recognition (NER): This is the task of identifying and classifying key elements (entities) in text into predefined categories. In this project, NER is analogous to the process of classifying a text span as an "Assignment" or identifying the "Due Date".
    \item Temporal Normalization: This step standardizes the extracted temporal expressions (dates and times) into a consistent format. In rule-based systems, this process may involve using dictionaries of temporal terms (weekdays, months, abbreviations) and substituting undefined values (like an ambiguous time frame) with a default value to maintain data integrity \cite{temp-expressions-clinical-narratives}.
\end{itemize}


\section{Prior Work}
Current automated solutions to mitigate this issue exhibit two critical limitations. First, existing tools such as Syllabuddy\cite{Syllabuddy} have been documented to suffer from poor accuracy when parsing, yielding unreliable results that undermine student trust. Second, the output scope of these tools is typically confined to generating calendar events, failing to provide the unified and actionable assignment lists that students require for effective prioritization and workload management. So, the fundamental challenge remains the lack of highly robust, accurate, and end to end automated system capable of reliably extracting all the relevant temporal data from diverse, unstructured syllabus documents and synthesizing it into a format directly integrable into student task management workflows \cite{zhu_pdfdataextractor_2022}. My project proposes to address this gap by developing a algorithmic framework that not only surpasses the accuracy limitations of existing systems but also delivers a comprehensive centralized assignment output, thereby significantly reducing the initial organizational burden and maximizing student time dedicated to academic engagement. 

[insert explanation of why i didnt do html tagging, why it is less efficient based on the few papers available on it]

\section{Methods}


\subsection{Model}
The process begins by parsing the PDF files to create structured text blocks. Tools like PDFMiner and pdfplumber handle this primitive conversion into text, which is an initial rule based approach to convert PDF files. The overall system utilizes a hybrid architecture that strategically combines the precision of rule based processing with the contextual flexibility of Machine Learning (ML) classifier. This approach is necessary because syllabi are highly semi structured documents, and generalized IE methods must be robust enough to cover diverse layouts found in educational settings. The methodology is divided into two primary phases: data preparation and the sequential information extraction process. 
\subsection{Data and Pre-processing}
Data acquisition was completed through manual collection. They spread across [] disciplines, and among differing levels between introductory classes and upper divisions. A key constraint for the dataset scope was the reliance on temperal expressions, which necessitated confining the syllabi to documents within the same time periods (quarter vs. semester system) and same time frame (eg. all Fall 2024) to ensure the parsed dates were consistent and meaningful. This standardization was crucial for the Temporal Normalization step, ensuring that ambiguous dates (like "Week 5") could be consistently interpreted or discarded. 
The creation of the Ground Truth dataset for all 60 syllabi required intensive manual input. This decision was unavoidable given the subjective and dynamic nature of classifying assignment entries, as well as the diverse nature of differing formats (tables, bullet points, lists, etc). Relying on manual verification, despite being unrealistic for a larger project, was required to establish a high quality gold standard that captured the nuances missed by early extraction. 
Data preparation focused on securing a representative sample and standardizing the text. The corpus of 60 syllabi utilized a  50/50 split (30 for training and 30 for testing) to ensure the validation set was large enough to reliably challenge the model's generalization across diverse formats. The process began with document parsing where text was retrieved from the PDF files. This step relies on the parsers to overcome the elementary nature of PDF extraction, which typically performs no structural analysis or identification of the logical role of text blocks. 

Preprocessing focused on converting this primitive text into usable features. This included Text Normalization steps such as lowercasing and removing punctuation and non-alphanumeric symbols. Crucially, boilerplate terms (e.g., "learning objectives," "grading policy") were removed to mitigate noise that could dilute the semantic features of true assignment entries. The Rule-Based aspect of the system was then applied through Regular Expressions (Regex) for Temporal Expression Recognition (TER) and subsequent Temporal Normalization. This step precisely identified dates in various formats, and when the year was absent, it was normalized to 2025, a process analogous to Temporal Normalization used in clinical narratives.

\section{Evaluation Metrics}
\subsection{Precision, Recall, and F1 Score}
The core evaluation uses three interdependent metrics, chosen because they are essential for assessing performance on a binary classification task (Assignment vs. Not Assignment) with severe class imbalance (few assignment lines versus many non-assignment lines). Using these metrics is justified by the literature, which applies them to evaluate temporal expression and event extraction systems.
\begin{itemize}
    \item Precision: $\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$. This measures correctness. It quantifies the proportion of all extracted assignment entries that were actually correct. A high Precision score minimizes False Positives (FP), meaning students are not misled by non-assignment text (like policy jargon) falsely flagged as a graded item.
    \item Recall: $\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$. This measures completeness. It quantifies the proportion of actual assignments in the Ground Truth that the system successfully identified. A high Recall score minimizes False Negatives (FN), the highest cost error for a scheduling application, as a missed assignment leads to a missed deadline.
    \item $\text{F}_{1} \text{Score}$: $\text{F}_{1} \text{Score} = 2 \times \frac{\text{Recall} \times \text{Precision}}{\text{Recall} + \text{Precision}}$. As the harmonic mean of Precision and Recall, the $\text{F}_{1} \text{Score}$ is the most critical metric for imbalanced data, providing a balanced measure of performance that penalizes models that favor one metric over the other.

\end{itemize}
\subsection{Evaluation Standard and Success Criteria}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Screenshot 2025-12-11 at 5.30.28 AM.png}
    \caption{Evaluation Standard and Success Criteria}
    \label{fig:placeholder}
\end{figure}


\section{Results and Discussion}
\subsection{Limitations}
The system's scope is limited to extracting the Assignment Name and Due Date. It does not perform full attribute assignment for other critical scheduling details, such as the time the assignment is due (e.g., 11:59 PM), the submission method (e.g., LMS vs. Email), or the point value of the assignment. Without these attributes, the extracted data is insufficient for directly creating a fully functional calendar event.
\subsubsection{Rule Based Faults}
The decision to use Regular Expressions (Regex) for Temporal Normalization introduces inherent fragility:
\begin{itemize}
    \item Format Brittleness: Regex rules are rigid, they only work for patterns explicitly coded into the system. If faculty introduce a slightly new or unexpected date format (e.g., "Due on Sept. 10th @ noon"), the system will fail to recognize the temporal expression, leading directly to a False Negative (a missed assignment), despite the classification model otherwise being highly accurate.
    \item Ambiguity in Relative Time: The system cannot reliably interpret relative temporal expressions (e.g., "the final presentation is three weeks after the midterm") without implementing an advanced event-chaining model, which falls outside the scope of this project. The system must rely on explicit dates.
\end{itemize}
\subsubsection{Constraint of Manual Data Sourcing}
The manual collection and creation of the Ground Truth for 60 syllabi, while necessary for initial development, is a massive practical limitation. This heavy manual input due to the subjective/dynamic nature of the project means the system cannot easily scale to hundreds or thousands of documents until a fully automated, low-labor method for continuous Ground Truth generation is devised.

\subsection{Directions for Future Research}
\subsubsection{Generalizability and Model Robustness}
The current system relies on basic parsing and date identification. Future work should implement techniques common in scientific literature extraction, such as those used in PDFDataExtractor, to perform explicit Section Detection and structural analysis to confirm the logical role of text blocks. This would improve confidence by confirming that an extracted date resides within an "Assignments" section rather than a "Course Schedule" section.
To achieve superior accuracy and resilience to complex layouts, research should investigate adopting an architecture that utilizes visual, spatial, and semantic information, moving toward an end-to-end architecture. This form of multimodal information extraction  can provide higher accuracy, as extraction often depends on the ability to interpret these different domains simultaneously.
To address the static bias of rule-based temporal systems, research should focus on developing algorithms capable of continuous learning. This would involve automatically detecting and adapting to new, common assignment terminology and scheduling patterns without requiring manual rule updates, maintaining the model's accuracy as syllabus formats evolve.
\subsubsection{Advanced Temporal Matching}

Research should explore associating the extracted temporal expressions with specific clinical events (analogous to assignment details in your context) to identify the full event structure.
Furthermore, implementing a mechanism for Temporal Normalization to handle ambiguous or relative temporal expressions (e.g., "the substitution of the undefined number of days") by using the syllabus's internal calendar or a "reference date" to calculate the absolute deadline.

\subsubsection{Incorporating Structural Classification for Accuracy}
One  direction for future research involves integrating  heading detection and structural classification, similar to research focusing on PDF text analysis \cite{budhiraja_supervised_2020}. By leveraging supervised learning models, future iterations could train a separate classifier to detect structural elements like headings and subheadings within the document structure. This approach enhances overall accuracy by using features related to text organization and formatting patterns, such as font size, boldness, word count, and text case.
\subsection{Practical and Societal Implications}
\subsubsection{Automation and Integration}
\begin{itemize}
    \item LMS Integration and API Development: The primary practical implication is the creation of an API Endpoint that can facilitate direct integration with student information systems (like Canvas or Blackboard). This automates the synchronization of assignment due dates from the syllabus to a student’s personal calendar and personal planner, eliminating the labor-intensive manual data abstraction required by both faculty and students.
    \item Institutional Analytics: By structuring disparate syllabus data, the tool enables institutions to auto-build databases of course requirements. This facilitates deeper analytics regarding workload consistency and curriculum mapping, enhancing the value of the data which would otherwise span many unstructured documents.
\end{itemize}
\subsubsection{Reducing Academic Inequity}
The automation contributes to mitigating local inequity by ensuring that complex, chaotic schedule data is translated into a consistent, machine-readable format. When the system accurately identifies and structures key data points, it ensures that students using assistive technologies receive reliable schedule information, mitigating the barriers created by poorly formatted or unstructured course documents.
\section{Ethical Considerations}
The system performs Named Entity Recognition (NER) to isolate and extract key text spans, a process necessary for temporal recognition. However, this method simplifies the original document, stripping the extracted date or assignment title of its broader contextual policy (e.g., late penalties or group work rules). If the system's output is relied upon as the sole source of truth, it can misrepresent the full assignment requirements, potentially leading to student confusion or disputes.
In addition, a primary risk stems from the system's reliance on rule-based processing for initial parsing and temporal recognition. This approach implicitly rewards faculty who conform their syllabus writing to a predictable, standardized format. Syllabi utilizing non-linear, narrative, or creative pedagogical styles may have their assignments consistently missed (creating False Negatives), introducing a systemic pressure toward extraction-friendly templates and potentially stifling academic freedom and diversity in pedagogical expression.
\subsection{Difficult Decisions and Algorithmic Bias}
As a hybrid system, it is susceptible to static bias embeded in its rule sets. If the initial rules were trained on data reflecting one dominant academic style or culture. the system may fail to recognize valid temporal expressions or assigned terminology common in diverse academic environments (eg. in different global regions or specialized academic fields). This perpetuates the norms of the data it was trained on, contributing to global and local inequity by creating a tool that served only a subset of academic cultures. 
The most difficult decision involved mitigating the severe class imbalance (few assignment lines vs. many schedule lines) present in the raw data. To achieve the high F-1 score necessary for a reliable product, the minority class (assignments) was aggressively over-sampled prior to training. This choice prioritized recall (completeness) and high-confidence extraction over using only raw, non-augmented data. However, it risks the model learning overly biased patterns from the small initial pool of assignment examples, particularly if those examples were not diverse among academic disciplines. 
By developing a  high-performance IE standard, the project risks increasing the technological gap between well-funded and under resourced institutions. The ML model requires well structured input to perform its classification accurately. Educational institutions that cannot afford updated document management systems or whose faculty utilize legacy formatting will experience lower reliability, thereby increasing the efficiency disparity compared to institutions whose data conforms neatly to the system's requirements.  
\section{Interlude: Using \LaTeX}

LaTeX (pronounced \textit{lah-teck} or \textit{lay-teck}), often stylized as {\LaTeX} and written as latex, is a document markup language and typesetting system.
Building on the {\TeX} language created by Donald Knuth in 1978, LaTeX provides additional commands for common document needs such as sections, figures, and bibliographies.
LaTeX is widely used in academia, especially in mathematical fields, due to how easy it is to write mathematical equations and its automatic management of references.
Since it is a markup language, LaTeX source files are written in plain text, which also makes it compatible with version control systems like git.

The most primitive syntax of LaTeX is the \textit{macro} or \textit{command}, which is always written with a backslash followed by the name of the command.
The {\LaTeX} glyph, for example, can be created with the command \texttt{\textbackslash LaTeX}.
Some commands take parameters, which are denoted in braces (e.g., \texttt{\textbackslash usepackage\{biblatex\}} will import the \texttt{biblatex} package), with some commands additionally accepting options in square brackets (e.g., \texttt{\textbackslash usepackage[style=numeric]\{biblatex\}}).
The \texttt{\textbackslash begin} and \texttt{\textbackslash end} commands are special, as they indicate the start and end of an \textit{environment}.
The content of a LaTeX document, for example, is surrounded by \texttt{\textbackslash begin\{document\}} and \texttt{\textbackslash end\{document\}}, indicating the text that should appear in the document.

\begin{figure}
    \centering
    \includegraphics[width=.95\linewidth]{recursion.png}
    \caption{
        The current page of this paper, as an image.
    }
    \label{fig:first-page}
\end{figure}

Beyond the basic syntax, much of learning LaTeX is learning the different commands and environments that exist.
For example, text can be made \textbf{bold} with \texttt{\textbackslash textbf}, \textit{italic} with \texttt{\textbackslash textit}, and \texttt{monospaced} with \texttt{\textbackslash texttt} (the \texttt{tt} stands for for ``teletype'').
Single dollar signs denote inline equations, so \texttt{\$E = mc\textasciicircum 2\$} will be rendered as $E = mc^2$.
Double dollar signs will render the equation in display mode, which we can see with the quadratic formula:

$$\frac{{-b \pm \sqrt {b^2 - 4ac} }}{{2a}}$$

There are also a handful of common environments:

\begin{itemize}
    \item \texttt{itemize} for bulletpoint lists
    \item \texttt{enumerate} for numbered lists
    \item \texttt{figure} for figures (see Figure \ref{fig:first-page})
    \item \texttt{table} and \texttt{tabular} for tables (see Table \ref{tbl:timeline})
    \item \texttt{lstlisting} for code listings (see Listing \ref{lst:python-hello-world})
\end{itemize}

\begin{lstlisting}[
    language=Python,
    caption={Hello, World! in Python},
    label={lst:python-hello-world},
    float
]
def hello():
    print('hello, world!')
\end{lstlisting}

This covers the most frequently used commands, but as you might have already inferred, LaTeX is a vast and deep system, and can easily be overwhelming.
We recommended reading through the as reference.
Beyond that, you should examine and play with the source of this document to gain working proficiency with LaTeX.

One final note on writing in LaTeX.
Since LaTeX is only a markup language, the source files must be compiled into a viewable form, most commonly into a PDF file.
The source of this document is made up of several files, each with its own purpose:
\begin{itemize}
    \item \texttt{document.tex} - The main LaTeX file containing the contents of the document.
    \item \texttt{oxycomps.sty} - A style file with settings for what the document should look like.
    \item \texttt{references.bib} - A list of bibliography items.
    \item Other files containing images, build instructions, etc.
\end{itemize}
Manual compilation of these files is somewhat esoteric, as it requires multiple uses of the \texttt{pdflatex} and \texttt{biblatex} commands.
Instead, it is much easier to use tools such as \texttt{pdflatex} or \texttt{latexmk} (in the terminal) or Overleaf (online) for compilation.
We have also provided a \texttt{Makefile} which will automatically update the document as necessary; the use of makefiles is beyond the scope of this document, but see 

\begin{comment}
No definition citations, unless the term itself is in dispute
Separate problem background from technical background
    Unclear if games and apps require much technical background
    The general structure of the framework might be better suited for the Architecture Overview section
        Eg. Flask uses decorators to associate functions with URLs
        Eg. Unity has scripts associated with objects and specific triggers, such as walking into an area, pressing a button, etc.
    Maybe a better name is "algorithmic background"?
        Should explore what does and doesn't count
            All ML counts
            App and game frameworks do not
        Framework vs. library?
            I like the idea of [inversion of control](https://martinfowler.com/bliki/InversionOfControl.html), but that may be too abstract for students to understand
        Heuristic: is understanding that system necessary to understand the results?
            Ie. How Flask or Unity works doesn't influence whether the app/game is useful/fun/engaging
            But how (say) linear regression works is highly relevant for why the results match/don't match the actual values
\end{comment}

\section{The Oxy CS Comps Paper}
\label{sec:paper}

This section will walk through the different sections of the comps final paper.
Note that your specific paper may not have all these sections; for example, it is common for app design projects to combine the methods and evaluation sections, to better match the iterative design process.
Nonetheless, your paper should have all of the \textit{content} described here.

\subsection{Introduction and Problem Context}

This section should motivate why the project is interesting both to you and to the computer science community or the general public.
You should also justify the difficult of the project.
As a rough guideline, your project should be either narrow but deep in a subfield of CS, or broadly reaching across subfields without being too shallow.
It should be comparable to the amount of work/content in an upper-level elective.

\subsection{Technical Background}

This section introduces the technical knowledge necessary to understand your project, including any terminology and algorithms.
You should assume that the reader is a CS undergraduate like yourself, but not necessarily familiar with AI/ML/HCI/apps/video games/etc.

\subsection{Prior Work}

This section describes of related and/or existing work.
This could be scientific or scholarly, but may also be a survey of existing products/games.
The goal of this section is to put your project in the context of what has already been done.

\subsection{Methods}

This section describes what exactly you will be working on.
What are you building? How will it combine/incorporate ideas from the literature? Be specific about what you will be doing: talk about the specific algorithm you will implement/use, the specific dataset/platform/API, and what the outcome of your project will look like.
All of these decisions should be justified as well.

\subsection{Evaluation Metrics}

This section describes how you will evaluate your project.
What will you be measuring, and how will you measure it?
You might think about what would result in an F, a C, or an A for comps.
Alternately, think about what are the minimal requirements for passing the class, what you might do if you had more time and resources, and what the best case scenario would be if everything went swimmingly.

\subsection{Results and Discussion}

limitations

\subsection{Ethical Considerations}

Are there any ethical concerns that might arise from your project?
You might think about whether your project perpetuates societal inequity (or could be used by others to do so), whether the data/platforms you are using is collected with informed consent and free of bias, and whether you might be subject to technological solutionism instead of working support/better the public infrastructure.
Include a discussion of how you plan to mitigate these issues in your project.

\subsection{Timeline}

A timeline of major milestones, with specific items to be completed by specific dates/months.
Note that this timeline must start over the summer; otherwise you will unlikely have enough time to complete a project of the expected scope.
As part of the discussion around your timeline, talk about what you already know that would help you with the project, and what you expect to have to learn to be successful.
Include programming languages, technical concepts, as well as processes (e.g., user testing).

\subsection{Replication Instructions}

This section will demonstrate that you have thought through the basics of how your code will work. You should include a diagram of the overall data flow of your program, including what the inputs and outputs of each component will be, and how they will be represented.

\subsection{Code Overview}

\subsection{Appendices}

\section{Project-Specific Guidance}

\subsection{Machine Learning}

\subsection{Mobile and Web Apps}

\subsection{Games and Experiences}

\section{Tips and Advice}

\subsection{The Academic Voice}

Although you may already have a lot of practice with college-level writing, the 
\printbibliography

\appendix

\clearpage

\onecolumn

\section{Paper Rubric}
\begin{landscape}\small
\begin{longtable}{p{0.19\linewidth} | p{0.19\linewidth} | p{0.19\linewidth} | p{0.19\linewidth} | p{0.19\linewidth}}
\textbf{CONTENT AREA}
    & \textbf{Excellent}
    & \textbf{Good}
    & \textbf{Marginal}
    & \textbf{Unacceptable} \\
\toprule \endhead
\textbf{Problem Statement} - What problem are you solving? Why is it important?
    & \cellcolor{excellent} The problem to be solved is clearly stated, and its importance and value clearly laid out. All relevant societal context is discussed, as it relates to and will influence the project.
    & \cellcolor{good} The problem to be solved is clearly stated, but its importance and value overstated. Most relevant societal context is discussed, but may need clarification on how it relates to or will influence the project.
    & \cellcolor{marginal} The problem to be solved is only vaguely stated, and its importance and value overstated. Significant social context is missing, or the context does not readily relate to the project.
    & \cellcolor{unacceptable} There is no clear problem to be solved, with minimal explanation of the importance/value of the project. Little to no societal context is presented. \\
\midrule
\textbf{Technical Background} - What does someone need to know to understand the problem?
    & \cellcolor{excellent} All technical terms are defined as they are used and understandable for the average CS student. Relevant mathematical and algorithmic concepts are introduced and explained.
    & \cellcolor{good} Most technical terms are defined as they are used, but may require additional study for the average CS student. Some relevant mathematical and algorithmic concepts are missing.
    & \cellcolor{marginal} Most technical terms are defined, or include jargon not understandable by the average CS student. Many relevant mathematical and algorithmic concepts are missing.
    & \cellcolor{unacceptable} The importance/value of the project is never explained. Many technical terms are undefined, or include jargon only the author understands. The necessary mathematical and algorithmic concepts are missing. \\
\midrule
\textbf{Prior Work} - How have other people solved similar problems? What inspirations are you drawing from?
    & \cellcolor{excellent} All literature/prior work cited is relevant, and no relevant literature/work is missing. Papers are discussed in thematic groups, highlighting how they are connected. The discussion highlights how the literature influenced the project.
    & \cellcolor{good} The majority of the literature cited is relevant, but may be missing important areas of review. Papers are discussed in thematic groups, but with missing connections. Irrelevant details may be included in the description and key contributions to the project may be missing.
    & \cellcolor{marginal} Most of the literature/work cited is relevant, but many are not or may be missing. Papers are discussed in thematic groups, but with little or no connections. Descriptions include many irrelevant details, with unclear or missing contributions to the project.
    & \cellcolor{unacceptable} Most of the literature/work cited is irrelevant to the project. Papers are discussed individually with minimal connections. Significant or foundational work in the area is not included. Descriptions include irrelevant details or incorrect claims. \\
\midrule
\textbf{Methods} - How did you go about solving the problem? What did you do?
    & \cellcolor{excellent} The approach to the project is clearly laid out, and justified with respect to literature and the goals of the project. Intermediate decisions are explained and evidence for choosing between approaches presented. Prior and current work is clearly delineated.
    & \cellcolor{good} The approach to the project is laid out and justified, with minor details missing. Intermediate decisions may be missing. Alternative approaches are listed, but not the evidence for rejecting them. Prior and current work is delineated, with some areas of confusion
    & \cellcolor{marginal} The approach to the project is laid out, with insufficient detail. Intermediate decisions are not mentioned. The methods are only barely justified. No consideration to alternate approaches is given. The boundary between prior and current work is unclear.
    & \cellcolor{unacceptable} No methods, or methods that do not correspond to what was actually done. Intermediate decisions are unexplained and inexplicable. Prior and current work is muddled, or work is overstated or plagiarized. \\
\midrule
\textbf{Evaluation Metrics} - How will we know that the problem has been solved?
    & \cellcolor{excellent} The metrics used and method of collection are appropriate, clearly explained, and justified with respect to the literature and the problem to be solved. Other metrics and why they are not used are discussed.
    & \cellcolor{good} The metrics used and method of collection are appropriate and clearly explained, although the justification may need work. Some other metrics are considered, but with weak reasons for not using them.
    & \cellcolor{marginal} The metrics used and method of collection are marginally appropriate, but not clearly explained and with minimal justification. No other metrics were considered.
    & \cellcolor{unacceptable} No metrics were given, or the specific method of calculation unclear. The metrics are not justified or do not match the goals of the project. \\
\midrule
\textbf{Results and Discussion} - Did you actually solve the problem? Why/Why not?
    & \cellcolor{excellent} The stated metrics are used and the results explained with respect to the methods. Alternate explanations and caveats to the results are explored. The results are connected to the goals of the project.
    & \cellcolor{good} The stated metrics are used and the results explained, although they may not be verified. Some caveats are listed, but alternate explanations are not considered. The results are connected to the goals of the project.
    & \cellcolor{marginal} The stated metrics are used, but with no explanation of why those results occurred. Little to no caveats or alternate explanations are explored. The results have minimal connection to the goals of the project.
    & \cellcolor{unacceptable} No results, or results that do not match the evaluation metrics or the project goals. \\
\midrule
\textbf{Ethical Considerations} - Why might people want to be careful about your project? What difficult decisions did you have to make?
    & \cellcolor{excellent} The project is considered both in its complete technological and societal context. Issues of bias and diversity are explored in detail, and potential contributions to global and local inequity examined. Relevant literature is cited.
    & \cellcolor{good} Both technological and societal context of the project is explored, although some considerations may be missing. Issues of bias and diversity are explored, as are potential contributions to global and local inequity. Little to no relevant literature is cited.
    & \cellcolor{marginal} Only one of technological or societal context is considered, in insufficient detail. Issues of bias and diversity are explored in brief, as are contributions to global/local inequity. No relevant literature is cited. 
    & \cellcolor{unacceptable} Ethical considerations are non existent or perfunctory. Issues of bias and diversity are glossed over or ignored, as are potential contributions to global and local inequity. Little to no relevant literature is cited. \\
\midrule
\textbf{Replication Instructions} - How would someone else use your project? (this should be an appendix to your paper; this does not count towards your page requirement/limit)
    & \cellcolor{excellent} All software used by the project is listed, with instructions that successfully allow another CS student to execute it. Particular attention is paid to future-proving the instructions (eg. package version, VMs, etc.)
    & \cellcolor{good} All software used by the project is listed, with instructions that successfully allow another CS student to execute it. Reproduction may require additional steps (eg. unguided installation of software).
    & \cellcolor{marginal} Some software or steps for reproducing the project are missing or incorrect, requiring minor fixes. Reproduction may require additional steps (eg. unguided installation of software).
    & \cellcolor{unacceptable} Some software or steps for reproducing the project are missing or incorrect, requiring major fixes or debugging. Additional requirements may be missing entirely and uninferable from what is provided. \\
\midrule
\textbf{Code Architecture Overview} - How would someone else extend your project? (this should be an appendix to your paper; this does not count towards your page requirement/limit)
    & \cellcolor{excellent} The organization of the code is clearly laid out/diagrammed, with justification. Another developer could use the overview to extend or debug the project.
    & \cellcolor{good} The organization of the code with incomplete justification. Another developer could discuss the merits of the code, but not contribute.
    & \cellcolor{marginal} Boundaries between project components are awkward. The text is descriptive of the organization but does not aid understanding. Another developer will have trouble understanding the code.
    & \cellcolor{unacceptable} Boundaries between project components are bizarre or non-existent. Another developer is better off starting from scratch. \\
\end{longtable}
\end{landscape}

\end{document}
